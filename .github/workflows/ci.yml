name: YouTube ELT Pipeline CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.10"
  POSTGRES_VERSION: "13"

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: airflow
          POSTGRES_USER: airflow
          POSTGRES_DB: youtube_dw
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Set up environment variables
        run: |
          echo "YOUTUBE_API_KEY=test_key" >> $GITHUB_ENV
          echo "TARGET_CHANNEL_HANDLE=@testchannel" >> $GITHUB_ENV
          echo "JSON_OUTPUT_PATH=./data/raw" >> $GITHUB_ENV
          echo "PGHOST=localhost" >> $GITHUB_ENV
          echo "PGPORT=5432" >> $GITHUB_ENV
          echo "PGDATABASE=youtube_dw" >> $GITHUB_ENV
          echo "PGUSER=airflow" >> $GITHUB_ENV
          echo "PGPASSWORD=airflow" >> $GITHUB_ENV
          echo "DISABLE_DB=0" >> $GITHUB_ENV
          echo "ENABLE_SODA=0" >> $GITHUB_ENV

      - name: Wait for PostgreSQL
        run: |
          until pg_isready -h localhost -p 5432 -U airflow; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done

      - name: Run tests
        run: |
          export AIRFLOW_HOME=/tmp/airflow
          mkdir -p $AIRFLOW_HOME
          pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --tb=short

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy

      - name: Run Black
        run: black --check --diff .

      - name: Run isort
        run: isort --check-only --diff .

      - name: Run flake8
        run: flake8 src/ tests/ dags/ --count --select=E9,F63,F7,F82 --show-source --statistics

      - name: Run mypy
        run: mypy src/ --ignore-missing-imports

  airflow-dag-validation:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Airflow
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.7.0
          pip install -r requirements.txt

      - name: Validate DAGs
        run: |
          export AIRFLOW_HOME=/tmp/airflow
          mkdir -p $AIRFLOW_HOME
          python -c "
          import os
          os.environ['AIRFLOW_HOME'] = '/tmp/airflow'
          from airflow.models import DagBag
          dag_bag = DagBag(include_examples=False)
          print(f'Found {len(dag_bag.dags)} DAGs')
          for dag_id, dag in dag_bag.dags.items():
              print(f'DAG {dag_id}: {len(dag.tasks)} tasks')
          assert len(dag_bag.import_errors) == 0, f'DAG import errors: {dag_bag.import_errors}'
          print('All DAGs validated successfully!')
          "

  docker-build:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          docker build -t youtube-elt-pipeline:latest .

      - name: Test Docker image
        run: |
          docker run --rm youtube-elt-pipeline:latest python -c "
          import sys
          sys.path.append('/usr/local/airflow/src')
          from db_tasks import load_to_staging, transform_core
          print('Docker image test passed!')
          "

  security-scan:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"
          severity: "CRITICAL,HIGH"

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: "trivy-results.sarif"
